{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94e6ceaf-9dcf-4db9-abf4-8b02e228b565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.12.0-cp313-cp313-win_amd64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\jaini\\anaconda3\\lib\\site-packages (from faiss-cpu) (2.1.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\jaini\\anaconda3\\lib\\site-packages (from faiss-cpu) (24.2)\n",
      "Downloading faiss_cpu-1.12.0-cp313-cp313-win_amd64.whl (18.2 MB)\n",
      "   ---------------------------------------- 0.0/18.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/18.2 MB 2.6 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 1.6/18.2 MB 4.1 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 2.4/18.2 MB 4.2 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 3.1/18.2 MB 4.1 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 4.2/18.2 MB 4.2 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 5.0/18.2 MB 4.1 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 5.8/18.2 MB 4.1 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 6.8/18.2 MB 4.1 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 7.6/18.2 MB 4.1 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 8.4/18.2 MB 4.1 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 9.4/18.2 MB 4.1 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 10.2/18.2 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 11.3/18.2 MB 4.1 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 12.1/18.2 MB 4.1 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 12.8/18.2 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 13.9/18.2 MB 4.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 14.7/18.2 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 15.7/18.2 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 16.5/18.2 MB 4.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 17.3/18.2 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.2/18.2 MB 4.1 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b38feb4-246e-4294-99d5-5f90205dd0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 422 embeddings\n",
      "FAISS index built with 422 vectors\n",
      "‚úÖ Index saved as E:\\Companion-AI\\data\\vectorstore\\faiss_index.bin\n",
      "‚úÖ Metadata saved as E:\\Companion-AI\\data\\vectorstore\\faiss_metadata.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "EMBEDDINGS_FILE = r\"E:\\Companion-AI\\data\\vectorstore\\embeddings.json\"\n",
    "INDEX_FILE = r\"E:\\Companion-AI\\data\\vectorstore\\faiss_index.bin\"\n",
    "METADATA_FILE = r\"E:\\Companion-AI\\data\\vectorstore\\faiss_metadata.json\"\n",
    "\n",
    "\n",
    "# 1. Load embeddings\n",
    "with open(EMBEDDINGS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(data)} embeddings\")\n",
    "\n",
    "# 2. Convert to numpy array (float32 for FAISS)\n",
    "embeddings = np.array([d[\"embedding\"] for d in data]).astype(\"float32\")\n",
    "\n",
    "# 3. Build FAISS index\n",
    "dimension = embeddings.shape[1]  # embedding vector size\n",
    "index = faiss.IndexFlatL2(dimension)  # L2 distance - Euclidean Distance\n",
    "index.add(embeddings)\n",
    "print(f\"FAISS index built with {index.ntotal} vectors\")\n",
    "\n",
    "# 4. Save index\n",
    "faiss.write_index(index, INDEX_FILE)\n",
    "\n",
    "# 5. Save metadata separately (file_name + chunk_id for lookup)\n",
    "metadata = [{\"file_name\": d[\"file_name\"], \"chunk_id\": d[\"chunk_id\"]} for d in data]\n",
    "with open(METADATA_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metadata, f)\n",
    "\n",
    "print(f\"‚úÖ Index saved as {INDEX_FILE}\")\n",
    "print(f\"‚úÖ Metadata saved as {METADATA_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d94af8fc-a7a7-48e8-abf8-89778bb418c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded FAISS index with 422 vectors\n",
      "Loaded metadata for 422 chunks\n",
      "\n",
      "üîç Top Matches:\n",
      "1. File: LG_Fridge_2.txt, Chunk ID: 23, Distance: 0.9177261590957642\n",
      "2. File: LG_Fridge_3.txt, Chunk ID: 35, Distance: 1.0003751516342163\n",
      "3. File: LG_Fridge_2.txt, Chunk ID: 64, Distance: 1.0956792831420898\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# --- Paths ---\n",
    "INDEX_FILE = r\"E:\\Companion-AI\\data\\vectorstore\\faiss_index.bin\"\n",
    "METADATA_FILE = r\"E:\\Companion-AI\\data\\vectorstore\\faiss_metadata.json\"\n",
    "\n",
    "# --- Load FAISS index ---\n",
    "index = faiss.read_index(INDEX_FILE)\n",
    "\n",
    "# --- Load metadata ---\n",
    "with open(METADATA_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(f\"Loaded FAISS index with {index.ntotal} vectors\")\n",
    "print(f\"Loaded metadata for {len(metadata)} chunks\")\n",
    "\n",
    "# --- Function: embed query using NIM ---\n",
    "def embed_query(query_text):\n",
    "    url = \"http://172.16.5.50:8000/v1/embeddings\"   # GPU server NIM endpoint\n",
    "    payload = {\n",
    "        \"model\": \"nvidia/llama-3.2-nv-embedqa-1b-v2\",\n",
    "        \"input\": [query_text],\n",
    "        \"input_type\": \"query\"   # ‚ö†Ô∏è important: \"query\" here\n",
    "    }\n",
    "    response = requests.post(url, json=payload)\n",
    "    response.raise_for_status()\n",
    "    return np.array(response.json()[\"data\"][0][\"embedding\"], dtype=\"float32\").reshape(1, -1)\n",
    "\n",
    "# --- Run a test query ---\n",
    "query = \"Why is my fridge making a buzzing noise?\"\n",
    "query_vec = embed_query(query)\n",
    "\n",
    "# --- Search FAISS ---\n",
    "k = 3  # top results\n",
    "distances, indices = index.search(query_vec, k)\n",
    "\n",
    "print(\"\\nüîç Top Matches:\")\n",
    "for rank, idx in enumerate(indices[0]):\n",
    "    meta = metadata[idx]\n",
    "    print(f\"{rank+1}. File: {meta['file_name']}, Chunk ID: {meta['chunk_id']}, Distance: {distances[0][rank]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b51dbdb7-ca73-4404-a469-c52b6a1b88ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Top Matches with Text:\n",
      "\n",
      "1. File: LG_Fridge_2.txt, Chunk ID: 23, Distance: 0.9177261590957642\n",
      "   Text Preview: ...\n",
      "\n",
      "2. File: LG_Fridge_3.txt, Chunk ID: 35, Distance: 1.0003751516342163\n",
      "   Text Preview: ...\n",
      "\n",
      "3. File: LG_Fridge_2.txt, Chunk ID: 64, Distance: 1.0956792831420898\n",
      "   Text Preview: ...\n"
     ]
    }
   ],
   "source": [
    "# Load full embeddings data (with text) to retrieve content\n",
    "with open(r\"E:\\Companion-AI\\data\\vectorstore\\embeddings.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    embeddings_data = json.load(f)\n",
    "\n",
    "# Build a lookup dictionary: (file_name, chunk_id) -> text\n",
    "chunk_lookup = {\n",
    "    (item[\"file_name\"], item[\"chunk_id\"]): item.get(\"text\", \"\")\n",
    "    for item in embeddings_data\n",
    "}\n",
    "\n",
    "print(\"\\nüîç Top Matches with Text:\")\n",
    "for rank, idx in enumerate(indices[0]):\n",
    "    meta = metadata[idx]\n",
    "    text = chunk_lookup.get((meta['file_name'], meta['chunk_id']), \"‚ö†Ô∏è Text not found\")\n",
    "    print(f\"\\n{rank+1}. File: {meta['file_name']}, Chunk ID: {meta['chunk_id']}, Distance: {distances[0][rank]}\")\n",
    "    print(f\"   Text Preview: {text[:300]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4c0cd98-4451-49cf-a44a-e026e2ba2001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 262.1/981.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 981.5/981.5 kB 2.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six in c:\\users\\jaini\\anaconda3\\lib\\site-packages (from langdetect) (1.17.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993250 sha256=5294f10188eac84503d3e443102b361a3f7101e63c71fdce5e319d49aac92975\n",
      "  Stored in directory: c:\\users\\jaini solanki\\appdata\\local\\pip\\cache\\wheels\\eb\\87\\25\\2dddf1c94e1786054e25022ec5530bfed52bad86d882999c48\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: Building 'langdetect' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'langdetect'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n"
     ]
    }
   ],
   "source": [
    "pip install langdetect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f15bf7ff-3afe-4ac4-9bfc-1c4a0238e7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss, json, requests, numpy as np\n",
    "\n",
    "# --- Paths ---\n",
    "INDEX_FILE = r\"E:\\Companion-AI\\data\\vectorstore\\faiss_index.bin\"\n",
    "METADATA_FILE = r\"E:\\Companion-AI\\data\\vectorstore\\faiss_metadata.json\"\n",
    "EMBEDDINGS_FILE = r\"E:\\Companion-AI\\data\\vectorstore\\embeddings.json\"\n",
    "\n",
    "# --- Services ---\n",
    "EMBEDDING_URL = \"http://172.16.5.50:8000/v1/embeddings\"\n",
    "EMBED_MODEL = \"nvidia/llama-3.2-nv-embedqa-1b-v2\"\n",
    "\n",
    "# --- Load FAISS index + metadata ---\n",
    "index = faiss.read_index(INDEX_FILE)\n",
    "with open(METADATA_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    metadata = json.load(f)\n",
    "with open(EMBEDDINGS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    embeddings_data = json.load(f)\n",
    "chunk_lookup = {(d[\"file_name\"], d[\"chunk_id\"]): d.get(\"text\", \"\") for d in embeddings_data}\n",
    "\n",
    "# --- Helper: embed query ---\n",
    "def embed_query(query: str):\n",
    "    payload = {\"model\": EMBED_MODEL, \"input\": [query], \"input_type\": \"query\"}\n",
    "    r = requests.post(EMBEDDING_URL, json=payload)\n",
    "    r.raise_for_status()\n",
    "    return np.array(r.json()[\"data\"][0][\"embedding\"], dtype=\"float32\").reshape(1, -1)\n",
    "\n",
    "# --- Helper: retrieve top-k chunks ---\n",
    "def retrieve_top_k(query_vec, k=3):\n",
    "    D, I = index.search(query_vec, k)\n",
    "    results = []\n",
    "    for dist, idx in zip(D[0], I[0]):\n",
    "        meta = metadata[idx]\n",
    "        text = chunk_lookup.get((meta['file_name'], meta['chunk_id']), \"\")\n",
    "        results.append({\"file\": meta[\"file_name\"], \"chunk\": meta[\"chunk_id\"], \"distance\": float(dist), \"text\": text})\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f6eafd8-2231-4cd4-89aa-3c9bc51048b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINAL PROMPT TO SEND TO LLM ===\n",
      "\n",
      "You are a helpful assistant for appliance manuals. \n",
      "Use ONLY the information in the context below to answer the user's question.\n",
      "If the context does not contain the answer, say \"I don‚Äôt know\" and suggest safe next steps.\n",
      "\n",
      "CONTEXT:\n",
      "\n",
      "\n",
      "QUESTION:\n",
      "Why is my fridge making a buzzing noise?\n",
      "\n",
      "INSTRUCTIONS:\n",
      "- Answer clearly in simple language.\n",
      "- Provide step-by-step troubleshooting if applicable.\n",
      "- Always cite sources in this format: [SOURCE: filename#chunkID].\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect, DetectorFactory\n",
    "DetectorFactory.seed = 0  # ensures consistent results\n",
    "\n",
    "def build_prompt(query, retrieved_chunks, max_chars=1200):\n",
    "    \"\"\"\n",
    "    Build a prompt for the LLM using retrieved chunks, with language detection.\n",
    "    \"\"\"\n",
    "    context_parts = []\n",
    "    total_chars = 0\n",
    "    \n",
    "    for r in retrieved_chunks:\n",
    "        snippet = r[\"text\"].strip()\n",
    "        if not snippet:\n",
    "            continue\n",
    "        \n",
    "        # Detect language (try-catch to avoid errors on short/garbled text)\n",
    "        try:\n",
    "            lang = detect(snippet[:200])  # detect on first 200 chars\n",
    "        except:\n",
    "            lang = \"unknown\"\n",
    "        \n",
    "        lang_note = \"\"\n",
    "        if lang != \"en\":\n",
    "            lang_note = f\"\\n‚ö†Ô∏è NOTE: This chunk is in {lang.upper()} language.\"\n",
    "        \n",
    "        # Truncate long chunks\n",
    "        if len(snippet) > max_chars:\n",
    "            snippet = snippet[:max_chars] + \" ... (truncated)\"\n",
    "        \n",
    "        context_parts.append(f\"[SOURCE: {r['file']}#{r['chunk']}]{lang_note}\\n{snippet}\")\n",
    "        total_chars += len(snippet)\n",
    "        \n",
    "        if total_chars > 5000:  # stop if too large\n",
    "            break\n",
    "    \n",
    "    context_block = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant for appliance manuals. \n",
    "Use ONLY the information in the context below to answer the user's question.\n",
    "If the context does not contain the answer, say \"I don‚Äôt know\" and suggest safe next steps.\n",
    "\n",
    "CONTEXT:\n",
    "{context_block}\n",
    "\n",
    "QUESTION:\n",
    "{query}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Answer clearly in simple language.\n",
    "- Provide step-by-step troubleshooting if applicable.\n",
    "- Always cite sources in this format: [SOURCE: filename#chunkID].\n",
    "\"\"\"\n",
    "    return prompt.strip()\n",
    "\n",
    "# --- Example run ---\n",
    "user_query = \"Why is my fridge making a buzzing noise?\"\n",
    "\n",
    "query_vec = embed_query(user_query)\n",
    "retrieved_chunks = retrieve_top_k(query_vec, k=3)\n",
    "\n",
    "prompt = build_prompt(user_query, retrieved_chunks)\n",
    "\n",
    "print(\"=== FINAL PROMPT TO SEND TO LLM ===\\n\")\n",
    "print(prompt[:2000])  # preview first 2000 chars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d14865e-f7d8-4e3d-81f4-1f4fecdcb105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths found:\n",
      " embeddings.json -> E:\\Companion-AI\\data\\vectorstore\\embeddings.json\n",
      " faiss_index.bin -> E:\\Companion-AI\\data\\vectorstore\\faiss_index.bin\n",
      " faiss_metadata.json -> E:\\Companion-AI\\data\\vectorstore\\faiss_metadata.json\n",
      " chunks.json (fallback) -> E:\\Companion-AI\\data\\chunks\\chunks.json\n",
      "\n",
      "FAISS index loaded. ntotal vectors = 422\n",
      "Metadata entries = 422\n",
      "Loaded embeddings.json; entries = 422\n",
      "Loaded chunks.json with 422 entries.\n",
      "Built chunk_lookup from chunks.json.\n",
      "Sample chunk_lookup keys (up to 5): [('LG_Fridge_1.txt', 1), ('LG_Fridge_1.txt', 2), ('LG_Fridge_1.txt', 3), ('LG_Fridge_1.txt', 4), ('LG_Fridge_1.txt', 5)]\n",
      "Example text length for first sample (if any): 2906\n",
      "\n",
      "\n",
      "--- Running retrieval for query: Why is my fridge making a buzzing noise? ---\n",
      "Retrieved results count: 4\n",
      "1. LG_Fridge_2.txt#23 dist=0.9177 text_present=True preview: Rattling: Rattling noises may come from the Ô¨Çow of refrigerant, the water line, or items stored on top of the refrigerator. Whooshing: Popping: Contraction and expansion of the inside walls. Sizzling:\n",
      "2. LG_Fridge_3.txt#35 dist=1.0004 text_present=True preview: detect a person before it will operate the LED Lighting. If the flooring material is highly reflective, or if there are highly reflective objects in front of the product, the proximity sensor which co\n",
      "3. LG_Fridge_2.txt#64 dist=1.0957 text_present=True preview: La mayor√≠a de los nuevos sonidos son normales. Las superÔ¨Åcies duras, como el suelo, las paredes y los muebles, pueden hacer que los sonidos parezcan m√°s fuertes de lo que realmente son. A continuaci√≥n\n",
      "4. Sam_Fridge_3.txt#28 dist=1.1193 text_present=True preview: or the optimal temperature. ‚Ä¢ This happens if the ambient temperature is too low. Set the temperature higher. ‚Ä¢ Check if food containing a high portion of water is put in the coldest area of the fridg\n",
      "\n",
      "=== FINAL PROMPT (preview, first 2000 chars) ===\n",
      "\n",
      "You are a helpful assistant for appliance manuals.\n",
      "Use ONLY the information in the context below to answer the user's question.\n",
      "If the context does not contain the answer, say \"I don‚Äôt know\" and suggest safe next steps.\n",
      "\n",
      "CONTEXT:\n",
      "[SOURCE: LG_Fridge_2.txt#23] \n",
      "Rattling: Rattling noises may come from the Ô¨Çow of refrigerant, the water line, or items stored on top of the refrigerator. Whooshing: Popping: Contraction and expansion of the inside walls. Sizzling: Water dripping on the defrost heater during a defrost cycle. Vibrating Noise: If the side or back of the refrigerator is touching a cabinet or wall, some of the normal vibrations may make an audible sound. To eliminate the noise, make sure that the sides and back cannot vibrate against any wall or cabinet. Dripping: Water running into the drain pan during the defrost cycle. Pulsating or High-Pitched Sound: Your refrigerator is designed to run more efÔ¨Åciently to keep your food items at the desired temperature. The high efÔ¨Åciency compressor may cause your new refrigerator to run longer than your old one, but is still more energy efÔ¨Åcient than previous models. While the refrigerator is running, it is normal to hear a pulsating or high-pitched sound. Gurgling: As each cycle ends, you may hear a gurgling sound caused by the refrigerant Ô¨Çowing through the cooling system. Evaporator fan motor circulating the air through the refrigerator and freezer compartments. Air being forced over the condenser by the condenser fan. Ice compartment fan in the freezer on the left side of the refrigerator when the doors are open. 23 --- PAGE 25 (TEXT) --- s n o it u l o S s e s u a C el b is s o P m el b o r P Lights do not work. The power supply cord is unplugged. Firmly plug the cord into a live out ... (truncated)\n",
      "\n",
      "[SOURCE: LG_Fridge_3.txt#35] \n",
      "detect a person before it will operate the LED Lighting. If the flooring material is highly reflective, or if there are highly reflective objects in front of the product, the proximity sensor \n"
     ]
    }
   ],
   "source": [
    "# Diagnostic + Prompt Builder - self contained cell\n",
    "import os, json, requests, numpy as np, faiss\n",
    "from langdetect import detect, DetectorFactory\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# ---------- CONFIG: adjust if needed ----------\n",
    "EMBEDDING_URL = \"http://172.16.5.50:8000/v1/embeddings\"\n",
    "EMBED_MODEL = \"nvidia/llama-3.2-nv-embedqa-1b-v2\"\n",
    "\n",
    "# Candidate paths (tries these in order)\n",
    "EMB_JSON_CANDIDATES = [\n",
    "    r\"E:\\Companion-AI\\data\\vectorstore\\embeddings.json\",\n",
    "    \"../data/vectorstore/embeddings.json\",\n",
    "    \"/LAB/data/vectorstore/embeddings.json\",\n",
    "    \"/mnt/data/vectorstore/embeddings.json\"\n",
    "]\n",
    "INDEX_CANDIDATES = [\n",
    "    r\"E:\\Companion-AI\\data\\vectorstore\\faiss_index.bin\",\n",
    "    \"../data/vectorstore/faiss_index.bin\",\n",
    "    \"/LAB/faiss_index.bin\",\n",
    "    \"/mnt/data/faiss_index.bin\"\n",
    "]\n",
    "META_CANDIDATES = [\n",
    "    r\"E:\\Companion-AI\\data\\vectorstore\\faiss_metadata.json\",\n",
    "    \"../data/vectorstore/faiss_metadata.json\",\n",
    "    \"/LAB/faiss_metadata.json\",\n",
    "    \"/mnt/data/faiss_metadata.json\"\n",
    "]\n",
    "CHUNKS_CANDIDATES = [\n",
    "    r\"E:\\Companion-AI\\data\\chunks\\chunks.json\",\n",
    "    \"../data/chunks/chunks.json\",\n",
    "    \"/LAB/data/chunks/chunks.json\",\n",
    "    \"/mnt/data/chunks/chunks.json\"\n",
    "]\n",
    "\n",
    "def find_first_existing(paths):\n",
    "    for p in paths:\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "EMBEDDINGS_FILE = find_first_existing(EMB_JSON_CANDIDATES)\n",
    "INDEX_FILE = find_first_existing(INDEX_CANDIDATES)\n",
    "METADATA_FILE = find_first_existing(META_CANDIDATES)\n",
    "CHUNKS_FILE = find_first_existing(CHUNKS_CANDIDATES)\n",
    "\n",
    "print(\"Paths found:\")\n",
    "print(\" embeddings.json ->\", EMBEDDINGS_FILE)\n",
    "print(\" faiss_index.bin ->\", INDEX_FILE)\n",
    "print(\" faiss_metadata.json ->\", METADATA_FILE)\n",
    "print(\" chunks.json (fallback) ->\", CHUNKS_FILE)\n",
    "print(\"\")\n",
    "\n",
    "# Sanity checks\n",
    "if INDEX_FILE is None or METADATA_FILE is None:\n",
    "    raise RuntimeError(\"FAISS index or metadata file not found. Check INDEX_CANDIDATES/META_CANDIDATES paths.\")\n",
    "\n",
    "# Load index + metadata\n",
    "index = faiss.read_index(INDEX_FILE)\n",
    "with open(METADATA_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(\"FAISS index loaded. ntotal vectors =\", index.ntotal)\n",
    "print(\"Metadata entries =\", len(metadata))\n",
    "if index.ntotal != len(metadata):\n",
    "    print(\"‚ö†Ô∏è Warning: index.ntotal and len(metadata) differ. Make sure metadata matches the index used to build it.\")\n",
    "\n",
    "# Try to load embeddings.json (may or may not include text)\n",
    "embeddings_data = None\n",
    "if EMBEDDINGS_FILE:\n",
    "    try:\n",
    "        with open(EMBEDDINGS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            embeddings_data = json.load(f)\n",
    "        print(\"Loaded embeddings.json; entries =\", len(embeddings_data))\n",
    "    except Exception as e:\n",
    "        print(\"Could not load embeddings.json:\", e)\n",
    "else:\n",
    "    print(\"embeddings.json not found in candidate list.\")\n",
    "\n",
    "# Build chunk_lookup: prefer embeddings_data[text] if present, else fallback to chunks.json\n",
    "chunk_lookup = {}\n",
    "if embeddings_data and isinstance(embeddings_data, list) and embeddings_data and \"text\" in embeddings_data[0]:\n",
    "    print(\"Using 'text' field from embeddings.json to build chunk_lookup.\")\n",
    "    for d in embeddings_data:\n",
    "        chunk_lookup[(d[\"file_name\"], d[\"chunk_id\"])] = d.get(\"text\", \"\")\n",
    "else:\n",
    "    # fallback: try chunks.json (original chunks with text)\n",
    "    if CHUNKS_FILE:\n",
    "        try:\n",
    "            with open(CHUNKS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "                chunks_src = json.load(f)\n",
    "            print(\"Loaded chunks.json with\", len(chunks_src), \"entries.\")\n",
    "            # expected keys: file_name, chunk_id, text\n",
    "            for d in chunks_src:\n",
    "                key = (d.get(\"file_name\"), d.get(\"chunk_id\"))\n",
    "                chunk_lookup[key] = d.get(\"text\", \"\")\n",
    "            print(\"Built chunk_lookup from chunks.json.\")\n",
    "        except Exception as e:\n",
    "            print(\"Failed to load chunks.json for fallback:\", e)\n",
    "    else:\n",
    "        print(\"No chunks.json fallback found and embeddings.json had no 'text' field.\")\n",
    "        print(\"=> This is likely the reason your context was empty. You need the original chunk texts.\")\n",
    "        # continue with empty lookup (we'll show diagnostics)\n",
    "\n",
    "# Quick sample check: show a few keys\n",
    "sample_keys = list(chunk_lookup.keys())[:5]\n",
    "print(\"Sample chunk_lookup keys (up to 5):\", sample_keys)\n",
    "print(\"Example text length for first sample (if any):\", len(chunk_lookup.get(sample_keys[0], \"\")) if sample_keys else \"no data\")\n",
    "print(\"\")\n",
    "\n",
    "# --- Embedding & retrieval helpers ---\n",
    "import numpy as np\n",
    "def embed_query(query: str):\n",
    "    payload = {\"model\": EMBED_MODEL, \"input\": [query], \"input_type\": \"query\"}\n",
    "    try:\n",
    "        r = requests.post(EMBEDDING_URL, json=payload, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        emb = r.json()[\"data\"][0][\"embedding\"]\n",
    "        return np.array(emb, dtype=\"float32\").reshape(1, -1)\n",
    "    except Exception as e:\n",
    "        print(\"Embedding call failed:\", e)\n",
    "        return None\n",
    "\n",
    "def retrieve_top_k(query_vec, k=3):\n",
    "    if query_vec is None:\n",
    "        print(\"No query_vec provided.\")\n",
    "        return []\n",
    "    D, I = index.search(query_vec, k)\n",
    "    results = []\n",
    "    for dist, idx in zip(D[0], I[0]):\n",
    "        if idx < 0 or idx >= len(metadata):\n",
    "            print(\"Invalid index returned by FAISS:\", idx)\n",
    "            continue\n",
    "        meta = metadata[idx]\n",
    "        text = chunk_lookup.get((meta['file_name'], meta['chunk_id']), \"\")\n",
    "        results.append({\"file\": meta['file_name'], \"chunk\": meta['chunk_id'], \"distance\": float(dist), \"text\": text})\n",
    "    return results\n",
    "\n",
    "# --- Prompt builder with language detection ---\n",
    "def build_prompt(query, retrieved_chunks, max_chars=1500):\n",
    "    context_parts = []\n",
    "    total_chars = 0\n",
    "    for r in retrieved_chunks:\n",
    "        snippet = (r.get(\"text\") or \"\").strip()\n",
    "        if not snippet:\n",
    "            # Show where the missing text is coming from\n",
    "            snippet = \"\"\n",
    "            print(f\"‚ö†Ô∏è Missing text for {r['file']}#{r['chunk']} (distance {r['distance']:.3f})\")\n",
    "            continue\n",
    "        try:\n",
    "            lang = detect(snippet[:200])\n",
    "        except:\n",
    "            lang = \"unknown\"\n",
    "        lang_note = \"\" if lang == \"en\" else f\"\\n‚ö†Ô∏è NOTE: This chunk is in language '{lang}'\"\n",
    "        if len(snippet) > max_chars:\n",
    "            snippet = snippet[:max_chars] + \" ... (truncated)\"\n",
    "        context_parts.append(f\"[SOURCE: {r['file']}#{r['chunk']}] {lang_note}\\n{snippet}\")\n",
    "        total_chars += len(snippet)\n",
    "        if total_chars > 8000:\n",
    "            break\n",
    "    context_block = \"\\n\\n\".join(context_parts)\n",
    "    prompt = f\"\"\"You are a helpful assistant for appliance manuals.\n",
    "Use ONLY the information in the context below to answer the user's question.\n",
    "If the context does not contain the answer, say \"I don‚Äôt know\" and suggest safe next steps.\n",
    "\n",
    "CONTEXT:\n",
    "{context_block}\n",
    "\n",
    "QUESTION:\n",
    "{query}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Answer clearly in simple language.\n",
    "- Provide step-by-step troubleshooting if applicable.\n",
    "- Always cite sources in this format: [SOURCE: filename#chunkID].\n",
    "\"\"\"\n",
    "    return prompt.strip()\n",
    "\n",
    "# ---------- Run example ----------\n",
    "user_query = \"Why is my fridge making a buzzing noise?\"\n",
    "print(\"\\n--- Running retrieval for query:\", user_query, \"---\")\n",
    "qvec = embed_query(user_query)\n",
    "if qvec is None:\n",
    "    print(\"Embedding failed ‚Äî check EMBEDDING_URL and server. No retrieval possible.\")\n",
    "else:\n",
    "    retrieved = retrieve_top_k(qvec, k=4)\n",
    "    print(\"Retrieved results count:\", len(retrieved))\n",
    "    for i, r in enumerate(retrieved, 1):\n",
    "        txt_preview = (r['text'] or \"\")[:200].replace(\"\\n\",\" \")\n",
    "        print(f\"{i}. {r['file']}#{r['chunk']} dist={r['distance']:.4f} text_present={bool(r['text'])} preview: {txt_preview!s}\")\n",
    "    # Build prompt\n",
    "    prompt = build_prompt(user_query, retrieved)\n",
    "    print(\"\\n=== FINAL PROMPT (preview, first 2000 chars) ===\\n\")\n",
    "    print(prompt[:2000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414132c4-b8bf-40a2-a440-98a28b2d0da4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
